{
  "pipelineSpec": {
    "components": {
      "comp-data-prep": {
        "executorLabel": "exec-data-prep",
        "inputDefinitions": {
          "parameters": {
            "bucket": {
              "type": "STRING"
            },
            "bucket_folder": {
              "type": "STRING"
            },
            "data_path": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "nbr_documents": {
              "type": "INT"
            },
            "nbr_queries": {
              "type": "INT"
            }
          }
        }
      },
      "comp-evaluate-model": {
        "executorLabel": "exec-evaluate-model",
        "inputDefinitions": {
          "parameters": {
            "ann_index_type": {
              "type": "STRING"
            },
            "gcs_bucket": {
              "type": "STRING"
            },
            "gcs_data_folder": {
              "type": "STRING"
            },
            "local_data_folder": {
              "type": "STRING"
            },
            "local_model_path": {
              "type": "STRING"
            },
            "model_name": {
              "type": "STRING"
            },
            "nbr_documents": {
              "type": "INT"
            },
            "nbr_queries": {
              "type": "INT"
            },
            "train_model_output": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "deploy_decision": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-train-model": {
        "executorLabel": "exec-train-model",
        "inputDefinitions": {
          "parameters": {
            "gcs_bucket": {
              "type": "STRING"
            },
            "model_name": {
              "type": "STRING"
            },
            "model_path": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "dummy": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-data-prep": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "data_prep"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-storage' 'tqdm' 'more-itertools' 'kfp==1.8.22' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef data_prep(data_path: str, bucket: str, bucket_folder: str) -> NamedTuple(\n    \"Outputs\",\n    [\n        (\"nbr_queries\", int),\n        (\"nbr_documents\", int),\n    ],\n):\n    import logging\n    import os\n    import json\n    import csv\n    import urllib.request\n    import typing as t\n    from more_itertools import take\n    from zipfile import ZipFile\n    from tqdm import tqdm\n    from google.cloud import storage\n\n    def get_logger():\n        \"\"\"Sets up logger\"\"\"\n        logger = logging.getLogger(__name__)\n        logger.setLevel(logging.INFO)\n        handler = logging.StreamHandler()\n        handler.setFormatter(\n            logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n        )\n        logger.addHandler(handler)\n        return logger\n\n    def download_and_unzip_dbpedia(save_path: str, dataset: str = \"dbpedia-entity\") -> None:\n        \"\"\"\n        Downloads and unzips the DBpedia dataset.\n        DBpedia documentation: https://github.com/iai-group/DBpedia-Entity/\n        Download link comes from: https://github.com/beir-cellar/beir\n        \"\"\"\n        if not os.path.exists(f\"{save_path}/dbpedia-entity/corpus.jsonl\"):\n            url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n            fname = \"dbpedia-entity.zip\"\n\n            # download the model to the current working directory\n            urllib.request.urlretrieve(url, fname)\n\n            # extract to a new folder\n            if not os.path.exists(save_path):\n                os.makedirs(save_path)\n            with ZipFile(fname, \"r\") as zipf:\n                zipf.extractall(path=save_path)\n\n            # remove downloaded zip file\n            os.remove(fname)\n\n    def load_dbpedia(from_path: str) -> t.Tuple[\n        t.Dict[str, t.Dict[str, str]], t.Dict[str, str], t.Dict[str, t.Dict[str, int]]\n    ]:\n        \"\"\"\n        Loads the DBpedia dataset.\n        Code adapted from: https://github.com/beir-cellar/beir/blob/main/beir/datasets/data_loader.py\n\n        The queries dict is a key:value map of the query ID to the query text.  Here is an example of a query:\n        queries['TREC_Entity-9']\n\n        The query relations dict maps query IDs to a dict of document IDs and their relevancy scores (0, 1, 2), where\n        0 = irrelevant, 1 = relevant, and 2 = highly relevant.  Not all doc IDs appear in the dict results for a given\n        query.\n        qrels['TREC_Entity-9']\n\n        Corpus maps document IDs to a dict of texts and titles.\n        corpus['<dbpedia:Todd_Levy>']\n        \"\"\"\n        corpus, queries, qrels = {}, {}, {}\n\n        corpus_file = f\"{from_path}/dbpedia-entity/corpus.jsonl\"\n        queries_file = f\"{from_path}/dbpedia-entity/queries.jsonl\"\n        qrels_file = f\"{from_path}/dbpedia-entity/qrels/test.tsv\"\n\n        # load the corpus\n        num_lines = sum(1 for _ in open(corpus_file, 'rb'))\n        with open(corpus_file, encoding='utf8') as file:\n            for line in tqdm(file, total=num_lines):\n                line = json.loads(line)\n                corpus[line.get(\"_id\")] = {\n                    \"text\": line.get(\"text\"),\n                    \"title\": line.get(\"title\"),\n                }\n\n        # load the queries\n        with open(queries_file, encoding='utf8') as file:\n            for line in file:\n                line = json.loads(line)\n                queries[line.get(\"_id\")] = line.get(\"text\")\n\n        # load the query:doc relationships\n        reader = csv.reader(\n            open(qrels_file, encoding=\"utf-8\"),\n            delimiter=\"\\t\",\n            quoting=csv.QUOTE_MINIMAL\n        )\n        next(reader)\n\n        for _, row in enumerate(reader):\n            query_id, corpus_id, score = row[0], row[1], int(row[2])\n\n            if query_id not in qrels:\n                qrels[query_id] = {corpus_id: score}\n            else:\n                qrels[query_id][corpus_id] = score\n\n        return corpus, queries, qrels\n\n    def upload_to_cloud_storage(data: dict, bucket_name: str, folder_path: str, fname: str) -> None:\n        \"\"\"Uploads dictionary as JSON file to Google Cloud Storage bucket.\"\"\"\n        storage_client = storage.Client()\n        bucket = storage_client.bucket(bucket_name)\n        blob = bucket.blob(folder_path + \"/\" + fname)\n        blob.upload_from_string(json.dumps(data), timeout=int(60 * 5))\n        return\n\n    def validate(logger, queries: dict, qrels: dict, corpus: dict) -> set:\n        \"\"\"Runs data validation\"\"\"\n\n        # data validation: how many queries have no relevant documents?\n        queries_without_relevant_docs = 0\n        for _, relations in qrels.items():\n            total_irrelevant = 0\n            for _, label in relations.items():\n                if label == 0:\n                    total_irrelevant += 1\n            if total_irrelevant == len(relations):\n                queries_without_relevant_docs += 1\n        logger.info(\n            f\"Queries without relevant documents: \"\n            f\"{queries_without_relevant_docs} / {len(qrels)} = \"\n            f\"{round(100 * queries_without_relevant_docs / len(qrels), 2)}%\"\n        )\n\n        # data validation: how many queries have no query relations?\n        q_minus_qrel = len(set(queries) - set(qrels))\n        logger.info(\n            f\"Queries without query relations: \"\n            f\"{q_minus_qrel} / {len(set(queries))} = \"\n            f\"{round(100 * q_minus_qrel / len(set(queries)), 2)}%\"\n        )\n\n        # data validation: how many query relations have no query data?\n        qrel_minus_q = len(set(qrels) - set(queries))\n        logger.info(\n            f\"Query relations without queries: \"\n            f\"{qrel_minus_q} / {len(set(qrels))} = \"\n            f\"{round(100 * qrel_minus_q / len(set(qrels)), 2)}%\"\n        )\n\n        # data validation: how many documents in query relations have no document data?\n        unique_docs_in_qrels = set()\n        for _, rels in qrels.items():\n            unique_docs_in_qrels = unique_docs_in_qrels.union(set(rels))\n        qrel_minus_docs = len(unique_docs_in_qrels - set(corpus))\n        logger.info(\n            f\"Documents in query relations without document data: \"\n            f\"{qrel_minus_docs} / {len(unique_docs_in_qrels)} = \"\n            f\"{round(100 * qrel_minus_docs / len(unique_docs_in_qrels), 2)}%\"\n        )\n\n        # data validation: how many documents have no query relations?\n        docs_minus_qrels = len(set(corpus) - unique_docs_in_qrels)\n        logger.info(\n            f\"Documents without query relations: \"\n            f\"{docs_minus_qrels} / {len(set(corpus))} = \"\n            f\"{round(100 * docs_minus_qrels / len(set(corpus)), 2)}%\"\n        )\n\n        # assertion should be True if data validation counts are correct\n        assert len(set(corpus)) - len(unique_docs_in_qrels) == docs_minus_qrels\n\n        return unique_docs_in_qrels\n\n    def clean_data(\n            logger,\n            queries: dict,\n            qrels: dict,\n            corpus: dict,\n            unique_docs_in_qrels: set\n    ) -> t.Tuple[dict, dict]:\n        \"\"\"Cleans data\"\"\"\n\n        # remove documents without query relations - they will be of no use for evaluation\n        corpus = {k: v for k, v in corpus.items() if k in unique_docs_in_qrels}\n        logger.info(f\"New corpus size: {len(corpus)}\")\n\n        # remove queries without query relations - they will be of no use for evaluation\n        queries = {k: v for k, v in queries.items() if k in set(qrels)}\n        logger.info(f\"New query count: {len(queries)}\")\n\n        return queries, corpus\n\n    def save_data_to_cloud_storage(\n            queries: dict, qrels: dict, corpus: dict, gcs_bucket: str, gcs_folder_path: str\n    ) -> None:\n        \"\"\"Saves the data to be used for evaluation to Cloud Storage bucket\"\"\"\n        upload_to_cloud_storage(data=queries, bucket_name=gcs_bucket, folder_path=gcs_folder_path, fname=\"queries.json\")\n        upload_to_cloud_storage(data=qrels, bucket_name=gcs_bucket, folder_path=gcs_folder_path, fname=\"qrels.json\")\n        upload_to_cloud_storage(data=corpus, bucket_name=gcs_bucket, folder_path=gcs_folder_path, fname=\"corpus.json\")\n\n    logger = get_logger()\n    download_and_unzip_dbpedia(save_path=data_path)\n    corpus, queries, qrels = load_dbpedia(from_path=data_path)\n    unique_docs_in_qrels = validate(logger=logger, queries=queries, qrels=qrels, corpus=corpus)\n    queries, corpus = clean_data(\n        logger=logger,\n        queries=queries,\n        qrels=qrels,\n        corpus=corpus,\n        unique_docs_in_qrels=unique_docs_in_qrels\n    )\n    save_data_to_cloud_storage(\n        queries=queries,\n        qrels=qrels,\n        corpus=corpus,\n        gcs_bucket=bucket,\n        gcs_folder_path=bucket_folder\n    )\n\n    nbr_queries = len(qrels)\n    nbr_documents = len(corpus)\n    return (nbr_queries, nbr_documents,)\n\n"
            ],
            "image": "python:3.9-slim"
          }
        },
        "exec-evaluate-model": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "evaluate_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-storage==2.9.0' 'sentence-transformers==2.2.2' 'scikit-learn==0.24.2' 'pandas==1.3.4' 'kfp==1.8.22' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef evaluate_model(\n        model_name: str,\n        local_model_path: str,\n        local_data_folder: str,\n        gcs_bucket: str,\n        gcs_data_folder: str,\n        ann_index_type: str,\n        nbr_queries: int,\n        nbr_documents: int,\n        train_model_output: str,\n        metrics: dsl.Output[dsl.Metrics],\n) -> NamedTuple(\n    \"Outputs\",\n    [\n        (\"deploy_decision\", str),\n    ],\n):\n    import logging\n    import torch\n    import json\n    import typing as t\n    import numpy as np\n    import pandas as pd\n    from sentence_transformers import SentenceTransformer\n    from google.cloud import storage\n    from pathlib import Path\n    from scipy.spatial.distance import cdist\n    from sklearn.metrics import ndcg_score, label_ranking_average_precision_score, roc_auc_score\n\n    def get_logger():\n        \"\"\"Sets up logger\"\"\"\n        logger = logging.getLogger(__name__)\n        logger.setLevel(logging.INFO)\n        handler = logging.StreamHandler()\n        handler.setFormatter(\n            logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n        )\n        logger.addHandler(handler)\n        return logger\n\n    def download_files_from_cloud_storage(\n            gcs_client,\n            logger,\n            bucket: str,\n            bucket_folder: str,\n            local_folder: str,\n            exclude_list: t.List[str] = None\n    ) -> None:\n        \"\"\"Copies files from cloud storage folder to local folder.\"\"\"\n        bucket = gcs_client.get_bucket(bucket)\n        blobs = bucket.list_blobs(prefix=bucket_folder)\n        for blob in blobs:\n            if blob.name.endswith(\"/\"):\n                continue\n            path_split = blob.name.split(\"/\")\n            filename = path_split[-1]\n            directory = \"/\".join(path_split[:-1])\n            if exclude_list is not None and (directory in exclude_list or filename in exclude_list):\n                logger.info(f\"Skipping artifact {blob.name}\")\n                next\n            path = Path(f\"{local_folder}/{directory}\")\n            path.mkdir(parents=True, exist_ok=True)\n            logger.info(f\"Downloading {blob.name} to: {path}/{filename}\\n\")\n            blob.download_to_filename(f\"{path}/{filename}\")\n\n    def create_embeddings(model, model_embed_dim: int, queries: dict, corpus: dict) -> t.Tuple[np.array, np.array]:\n        \"\"\"Creates embeddings for the query strings and texts\"\"\"\n        q_embeddings = np.array(\n            model.encode(list(queries.values())),\n            dtype=np.float32\n        ).reshape(-1, model_embed_dim)\n\n        docs = [doc['text'] for doc in corpus.values()]\n        d_embeddings = np.array(\n            model.encode(docs),\n            dtype=np.float32\n        ).reshape(-1, model_embed_dim)\n\n        return q_embeddings, d_embeddings\n\n    def rescale(d: float, dampening_factor: float = 1):\n        \"\"\"\n        Inverts distance to be similarity.  Dividing by d + 1 ensures that 0 becomes 1 in the new scale.\n\n        :param d: distance measurement\n        :param dampening_factor: Increase this to flatten the curve and make larger values fall off slower\n            in the new scale.  This parameter should be tuned to what makes sense.  1.5 is a reasonable\n            starting value, see: https://www.desmos.com/calculator/eluirxagoz\n        \"\"\"\n        return (1 / (d + 1)) ** (1 / dampening_factor)\n\n    def score_relevance(query_vector: np.array, doc_embeddings: np.array, text_ids: list = None):\n        \"\"\"Computes scaled similarity (relevance) to a query vector for every document.\"\"\"\n        dist_to_query = cdist(query_vector.reshape(1, -1), doc_embeddings, metric='euclidean')\n        dist_to_query = rescale(d=dist_to_query[0])\n        if text_ids is not None:\n            output = {text_ids[i]: dist_to_query[i] for i in range(len(text_ids))}\n        else:\n            output = {i: dist_to_query[i] for i in range(len(dist_to_query))}\n        return output\n\n    def combine_labels_and_preds(qrels: dict, qrels_pred: dict, queries: dict, docs: dict) -> pd.DataFrame:\n        # assemble the labeled data\n        dfs = []\n        for query, relations in qrels.items():\n            doc_ids = list(relations)\n            labels = list(relations.values())\n            q_name = [query] * len(doc_ids)\n            q_text = queries[query]\n            doc_texts = [docs[did]['text'] for did in doc_ids]\n            qdf = pd.DataFrame({\n                \"query\": q_name,\n                \"query_text\": q_text,\n                \"doc_id\": doc_ids,\n                \"doc_text\": doc_texts,\n                \"label\": labels,\n            })\n            qdf = qdf.sort_values([\"label\", \"doc_id\"], ascending=False).reset_index(drop=True)\n            qdf['binary_label'] = np.where(qdf['label'] > 0, 1, 0)\n            dfs.append(qdf)\n        df = pd.concat(dfs, axis=0)\n\n        # assemble the predictions\n        pred_dfs = []\n        for query, relations in qrels_pred.items():\n            doc_ids = list(relations)\n            scores = list(relations.values())\n            q_name = [query] * len(doc_ids)\n            pdf = pd.DataFrame({\"query\": q_name, \"doc_id\": doc_ids, \"similarity_score\": scores})\n            pred_dfs.append(pdf)\n        pred_df = pd.concat(pred_dfs, axis=0)\n\n        # combine the predictions with the labels\n        final_df = pd.merge(left=df, right=pred_df, how='left', on=['query', 'doc_id'])\n        final_df['similarity_score'].fillna(0, inplace=True)\n        final_df.sort_values([\"query\", \"label\", \"similarity_score\"], ascending=False, inplace=True)\n        final_df.reset_index(drop=True, inplace=True)\n\n        return final_df\n\n    def score_results(df: pd.DataFrame):\n        \"\"\"\n        Split data into groups, score each query group's ranking by similarity score,\n        and re-combine.\n        \"\"\"\n        groups = [y for x, y in df.groupby('query')]\n        for group in groups:\n            group['ndcg_top10'] = ndcg_score([group['label']], [group['similarity_score']], k=10)\n            group['ndcg_top100'] = ndcg_score([group['label']], [group['similarity_score']], k=100)\n            group['ndcg'] = ndcg_score([group['label']], [group['similarity_score']])\n            group['lrap'] = label_ranking_average_precision_score(\n                [group['binary_label']], [group['similarity_score']]\n            )\n            group['roc_auc'] = roc_auc_score(group['binary_label'], group['similarity_score'])\n        return pd.concat(groups, axis=0)\n\n    def save_dataframe_to_cloud_storage(\n            gcs_client,\n            df: pd.DataFrame,\n            bucket_name: str,\n            folder_path: str,\n            fname: str\n    ) -> None:\n        \"\"\"Uploads dataframe as CSV file to Google Cloud Storage bucket.\"\"\"\n        bucket = gcs_client.bucket(bucket_name)\n        bucket.blob(f'{folder_path}/{fname}.csv').upload_from_string(df.to_csv(), 'text/csv')\n\n    logger = get_logger()\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    if device == \"cuda\":\n        torch.cuda.empty_cache()\n    logger.info(f\"Using device {device}\")\n\n    gcs_model_folder = f\"{model_name.lower()}\"\n    gcs_client = storage.Client()\n\n    # initializing the model from local download fails for a bug,\n    # see: https://github.com/UKPLab/sentence-transformers/issues/1590\n    # so we will cheat (since we know we have not trained the model) and load directly from HF hub\n    # when the bug is fixed, uncomment the code below to load from local\n\n    # download the trained model from gcs to the container\n    # download_files_from_cloud_storage(\n    #     gcs_client=gcs_client,\n    #     logger=logger,\n    #     bucket=gcs_bucket,\n    #     bucket_folder=gcs_model_folder,\n    #     local_folder=local_model_path,\n    #     exclude_list=[\"training_and_eval_artifacts\"]\n    # )\n\n    # load the model from local path\n    # model = SentenceTransformer(f\"{local_model_path}/{gcs_model_folder}\", device=device)\n    model = SentenceTransformer(model_name, device=device)\n\n    # download the eval data from gcs to the container\n    download_files_from_cloud_storage(\n        gcs_client=gcs_client,\n        logger=logger,\n        bucket=gcs_bucket,\n        bucket_folder=gcs_data_folder,\n        local_folder=local_data_folder\n    )\n\n    # load the eval data from local path\n    with open(f\"{local_data_folder}/{gcs_data_folder}/corpus.json\", \"r\") as file:\n        corpus = json.load(file)\n    with open(f\"{local_data_folder}/{gcs_data_folder}/queries.json\", \"r\") as file:\n        queries = json.load(file)\n    with open(f\"{local_data_folder}/{gcs_data_folder}/qrels.json\", \"r\") as file:\n        qrels = json.load(file)\n\n    # embed the queries and the documents\n    all_query_embed, all_doc_embed = create_embeddings(\n        model=model,\n        model_embed_dim=384,\n        queries=queries,\n        corpus=corpus\n    )\n\n    # map document IDs from the corpus to their index values in the corpus\n    doc_id_to_tid_map = {k: v for v, k in enumerate(list(corpus))}\n    tid_to_doc_id_map = dict(enumerate(list(corpus)))\n\n    # map query IDs to their embeddings\n    query_id_to_embed_map = {qid: all_query_embed[i, :] for i, qid in enumerate(list(queries))}\n\n    # map document IDs to their embeddings\n    doc_id_to_embedding_map = {did: all_doc_embed[i, :] for i, did in enumerate(list(corpus))}\n\n    # iterate over the query-doc relations and score them for relevancy\n    qrels_pred = {}\n    for query_id, doc_ids in qrels.items():\n        # get the query embedding\n        query_embed = query_id_to_embed_map[query_id].reshape(1, -1)\n\n        # get the document embeddings for the docs found in this query's relations\n        text_ids = [doc_id_to_tid_map[did] for did in list(doc_ids)]\n        doc_embed = np.concatenate([doc_id_to_embedding_map[did] for did in list(doc_ids)], axis=0).reshape(-1, 384)\n\n        # score the documents and map scores back to original doc IDs\n        query_doc_scores = score_relevance(query_vector=query_embed, doc_embeddings=doc_embed, text_ids=text_ids)\n        query_doc_scores = {tid_to_doc_id_map[k]: v for k, v in query_doc_scores.items()}\n\n        # store results for this query\n        q_res = {query_id: query_doc_scores}\n        qrels_pred.update(q_res)\n\n    df_pred = combine_labels_and_preds(\n        qrels=qrels,\n        qrels_pred=qrels_pred,\n        queries=queries,\n        docs=corpus\n    )\n\n    # make sure every query:doc pair has been accounted for\n    assert (len(df_pred) == sum([len(v) for v in qrels.values()]))\n    # score relevancy predictions against labels and check query:doc pairs again, as score_results produces a new df\n    df = score_results(df=df_pred)\n    assert (len(df) == sum([len(v) for v in qrels.values()]))\n\n    # metrics to be logged\n    metrics.log_metric(\"average_ndcg_top10\", df[\"ndcg_top10\"].mean())\n    metrics.log_metric(\"average_ndcg_top100\", df[\"ndcg_top100\"].mean())\n    metrics.log_metric(\"average_ndcg\", df[\"ndcg\"].mean())\n    metrics.log_metric(\"average_lrap\", df[\"lrap\"].mean())\n    metrics.log_metric(\"average_roc_auc\", df[\"roc_auc\"].mean())\n\n    # saving eval data with preds should be an optional step, but it could be useful for analysis in BigQuery\n    save_dataframe_to_cloud_storage(\n        gcs_client=gcs_client,\n        df=df,\n        bucket_name=gcs_bucket,\n        folder_path=gcs_data_folder,\n        fname=f\"eval_{model_name}.csv\"\n    )\n\n    # if the model performs well enough, it should be deployed\n    deploy_decision = \"true\"\n    return (deploy_decision,)\n\n"
            ],
            "image": "python:3.9-slim"
          }
        },
        "exec-train-model": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "train_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'sentence-transformers==2.2.2' 'kfp==1.8.22' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef train_model(model_name: str, model_path: str, gcs_bucket: str) -> NamedTuple(\n    \"Outputs\",\n    [\n        (\"dummy\", str),\n    ],\n):\n    import logging\n    import torch\n    from sentence_transformers import SentenceTransformer\n    from subprocess import call\n\n    def get_logger():\n        \"\"\"Sets up logger\"\"\"\n        logger = logging.getLogger(__name__)\n        logger.setLevel(logging.INFO)\n        handler = logging.StreamHandler()\n        handler.setFormatter(\n            logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n        )\n        logger.addHandler(handler)\n        return logger\n\n    def save_model_to_gcs(model_to_save, local_path: str, bucket_folder_uri: str) -> None:\n        \"\"\"\n        Saves the model locally, and then uses gsutil to copy the local directory to a gcs\n        bucket folder.\n        \"\"\"\n        # save the model locally first\n        model_to_save.save(local_path)\n\n        # copy files to bucket\n        call([\"gsutil\", \"cp\", \"-r\", f\"./${local_path}/*\", f\"${bucket_folder_uri}\"], shell=True)\n\n        # remove local directory\n        call([\"rm\", \"-R\", f\"${local_path}\"], shell=True)\n\n    logger = get_logger()\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = SentenceTransformer(model_name, device=device)\n\n    if device == \"cuda\":\n        torch.cuda.empty_cache()\n    logger.info(f\"Using device {device}\")\n\n    gcs_model_folder = f\"{model_name.lower()}\"\n    gcs_model_folder_uri = f\"gs://{gcs_bucket}/{gcs_model_folder}\"\n\n    # placeholder comment for future training\n\n    # save the model to gcs\n    save_model_to_gcs(\n        model_to_save=model,\n        local_path=model_path,\n        bucket_folder_uri=gcs_model_folder_uri\n    )\n\n    # To make sure downstream tasks are dependent on this one finishing,\n    # before they can begin, we will return a dummy string that downstream\n    # tasks will take as input.\n    dummy = \"done\"\n    return (dummy,)\n\n"
            ],
            "image": "google/cloud-sdk:slim"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "train-and-eval-ir-pipeline"
    },
    "root": {
      "dag": {
        "outputs": {
          "artifacts": {
            "evaluate-model-metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "metrics",
                  "producerSubtask": "evaluate-model"
                }
              ]
            }
          }
        },
        "tasks": {
          "data-prep": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-data-prep"
            },
            "inputs": {
              "parameters": {
                "bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "queryable-docs-artifacts-5024"
                    }
                  }
                },
                "bucket_folder": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ir_eval_data"
                    }
                  }
                },
                "data_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "data"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "data-prep"
            }
          },
          "evaluate-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-evaluate-model"
            },
            "dependentTasks": [
              "data-prep",
              "train-model"
            ],
            "inputs": {
              "parameters": {
                "ann_index_type": {
                  "componentInputParameter": "ann_index_type"
                },
                "gcs_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "queryable-docs-artifacts-5024"
                    }
                  }
                },
                "gcs_data_folder": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "ir_eval_data"
                    }
                  }
                },
                "local_data_folder": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "data"
                    }
                  }
                },
                "local_model_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "local_model"
                    }
                  }
                },
                "model_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "all-MiniLM-L6-v2"
                    }
                  }
                },
                "nbr_documents": {
                  "taskOutputParameter": {
                    "outputParameterKey": "nbr_documents",
                    "producerTask": "data-prep"
                  }
                },
                "nbr_queries": {
                  "taskOutputParameter": {
                    "outputParameterKey": "nbr_queries",
                    "producerTask": "data-prep"
                  }
                },
                "train_model_output": {
                  "taskOutputParameter": {
                    "outputParameterKey": "dummy",
                    "producerTask": "train-model"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "evaluate-model"
            }
          },
          "train-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-model"
            },
            "inputs": {
              "parameters": {
                "gcs_bucket": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "queryable-docs-artifacts-5024"
                    }
                  }
                },
                "model_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "all-MiniLM-L6-v2"
                    }
                  }
                },
                "model_path": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "local_model"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "train-model"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "ann_index_type": {
            "type": "STRING"
          },
          "project": {
            "type": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "evaluate-model-metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.22"
  },
  "runtimeConfig": {
    "gcsOutputDirectory": "gs://queryable-docs-artifacts-5024/all-minilm-l6-v2/training_and_eval_artifacts"
  }
}