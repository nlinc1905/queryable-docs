{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Evaluation Data\n",
    "\n",
    "This notebook gathers open source data that can be used to evaluate a model for information retrieval (IR) tasks.  We will use the [DBpedia](https://github.com/iai-group/DBpedia-Entity/) dataset, which is a subset of the Text REtrieval Conference [(TREC)](https://trec.nist.gov/data.html) dataset; a common benchmarking dataset for IR models.\n",
    "\n",
    "The approach we are using does not train per se: it simply calculates the distance between embeddings, and the sorted distances are evaluated against the labeled relevancy categories.  So all of the data gathered in this notebook is used only for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, csv\n",
    "import urllib.request\n",
    "import typing as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "from more_itertools import take\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "GCS_BUCKET = \"queryable-docs-artifacts-5024\"\n",
    "GCS_FOLDER_PATH = \"ir_eval_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip_dbpedia(save_path: str, dataset: str = \"dbpedia-entity\"):\n",
    "    \"\"\"\n",
    "    Downloads and unzips the DBpedia dataset.\n",
    "    DBpedia documentation: https://github.com/iai-group/DBpedia-Entity/\n",
    "    Download link comes from: https://github.com/beir-cellar/beir\n",
    "    \"\"\"\n",
    "    url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n",
    "    fname = \"dbpedia-entity.zip\"\n",
    "\n",
    "    # download the model to the current working directory\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "    # extract to a new folder\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    with ZipFile(fname, \"r\") as zipf:\n",
    "        zipf.extractall(path=save_path)\n",
    "\n",
    "    # remove downloaded zip file\n",
    "    os.remove(fname)\n",
    "\n",
    "\n",
    "def load_dbpedia(from_path: str) -> t.Tuple[t.Dict[str, t.Dict[str, str]], t.Dict[str, str], t.Dict[str, t.Dict[str, int]]]:\n",
    "    \"\"\"\n",
    "    Loads the DBpedia dataset.\n",
    "    Code adapted from: https://github.com/beir-cellar/beir/blob/main/beir/datasets/data_loader.py\n",
    "\n",
    "    The queries dict is a key:value map of the query ID to the query text.  Here is an example of a query:\n",
    "    queries['TREC_Entity-9']\n",
    "\n",
    "    The query relations dict maps query IDs to a dict of document IDs and their relevancy scores (0, 1, 2), where\n",
    "    0 = irrelevant, 1 = relevant, and 2 = highly relevant.  Not all doc IDs appear in the dict results for a given\n",
    "    query.\n",
    "    qrels['TREC_Entity-9']\n",
    "\n",
    "    Corpus maps document IDs to a dict of texts and titles.\n",
    "    corpus['<dbpedia:Todd_Levy>']\n",
    "    \"\"\"\n",
    "    corpus, queries, qrels = {}, {}, {}\n",
    "\n",
    "    corpus_file = f\"{from_path}/dbpedia-entity/corpus.jsonl\"\n",
    "    queries_file = f\"{from_path}/dbpedia-entity/queries.jsonl\"\n",
    "    qrels_file = f\"{from_path}/dbpedia-entity/qrels/test.tsv\"\n",
    "\n",
    "    # load the corpus\n",
    "    num_lines = sum(1 for i in open(corpus_file, 'rb'))\n",
    "    with open(corpus_file, encoding='utf8') as f:\n",
    "        for line in tqdm(f, total=num_lines):\n",
    "            line = json.loads(line)\n",
    "            corpus[line.get(\"_id\")] = {\n",
    "                \"text\": line.get(\"text\"),\n",
    "                \"title\": line.get(\"title\"),\n",
    "            }\n",
    "\n",
    "    # load the queries\n",
    "    with open(queries_file, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line)\n",
    "            queries[line.get(\"_id\")] = line.get(\"text\")\n",
    "\n",
    "    # load the query:doc relationships\n",
    "    reader = csv.reader(\n",
    "        open(qrels_file, encoding=\"utf-8\"),\n",
    "        delimiter=\"\\t\",\n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    next(reader)\n",
    "\n",
    "    for id, row in enumerate(reader):\n",
    "        query_id, corpus_id, score = row[0], row[1], int(row[2])\n",
    "\n",
    "        if query_id not in qrels:\n",
    "            qrels[query_id] = {corpus_id: score}\n",
    "        else:\n",
    "            qrels[query_id][corpus_id] = score\n",
    "\n",
    "    return corpus, queries, qrels\n",
    "\n",
    "\n",
    "def upload_to_cloud_storage(data: dict, bucket_name: str, folder_path: str, fname: str):\n",
    "    \"\"\"Uploads dictionary as JSON file to Google Cloud Storage bucket.\"\"\"          \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(folder_path + \"/\" + fname)\n",
    "    blob.upload_from_string(json.dumps(data), timeout=int(60*5))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4635922/4635922 [00:17<00:00, 269539.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# download the data\n",
    "if not os.path.exists(f\"{DATA_PATH}/dbpedia-entity/corpus.jsonl\"):\n",
    "    download_and_unzip_dbpedia(save_path=DATA_PATH)\n",
    "corpus, queries, qrels = load_dbpedia(from_path=DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries without relevant documents: 0 / 400 = 0.0%\n",
      "Queries without query relations: 67 / 467 = 14.35%\n",
      "Query relations without queries: 0 / 400 = 0.0%\n",
      "Documents in query relations without document data: 0 / 40724 = 0.0%\n",
      "Documents without query relations: 4595198 / 4635922 = 99.12%\n"
     ]
    }
   ],
   "source": [
    "# data validation: how many queries have no relevant documents?\n",
    "queries_without_relevant_docs = 0\n",
    "for k, v in qrels.items():\n",
    "    total_irrelevant = 0\n",
    "    for doc, label in v.items():\n",
    "        if label == 0:\n",
    "            total_irrelevant += 1\n",
    "    if total_irrelevant == len(v):\n",
    "        queries_without_relevant_docs += 1\n",
    "print(\n",
    "    f\"Queries without relevant documents: \"\n",
    "    f\"{queries_without_relevant_docs} / {len(qrels)} = \"\n",
    "    f\"{round(100 * queries_without_relevant_docs / len(qrels), 2)}%\"\n",
    ") \n",
    "\n",
    "# data validation: how many queries have no query relations?\n",
    "q_minus_qrel = len(set(queries) - set(qrels))\n",
    "print(\n",
    "    f\"Queries without query relations: \"\n",
    "    f\"{q_minus_qrel} / {len(set(queries))} = \"\n",
    "    f\"{round(100 * q_minus_qrel / len(set(queries)), 2)}%\"\n",
    ")\n",
    "\n",
    "# data validation: how many query relations have no query data?\n",
    "qrel_minus_q = len(set(qrels) - set(queries))\n",
    "print(\n",
    "    f\"Query relations without queries: \"\n",
    "    f\"{qrel_minus_q} / {len(set(qrels))} = \"\n",
    "    f\"{round(100 * qrel_minus_q / len(set(qrels)), 2)}%\"\n",
    ")\n",
    "\n",
    "# data validation: how many documents in query relations have no document data?\n",
    "unique_docs_in_qrels = set()\n",
    "for q, rels in qrels.items():\n",
    "    unique_docs_in_qrels = unique_docs_in_qrels.union(set(rels))\n",
    "qrel_minus_docs = len(unique_docs_in_qrels - set(corpus))\n",
    "print(\n",
    "    f\"Documents in query relations without document data: \"\n",
    "    f\"{qrel_minus_docs} / {len(unique_docs_in_qrels)} = \"\n",
    "    f\"{round(100 * qrel_minus_docs / len(unique_docs_in_qrels), 2)}%\"\n",
    ")\n",
    "\n",
    "# data validation: how many documents have no query relations?\n",
    "docs_minus_qrels = len(set(corpus) - unique_docs_in_qrels)\n",
    "print(\n",
    "    f\"Documents without query relations: \"\n",
    "    f\"{docs_minus_qrels} / {len(set(corpus))} = \"\n",
    "    f\"{round(100 * docs_minus_qrels / len(set(corpus)), 2)}%\"\n",
    ")\n",
    "\n",
    "# assertion should be True if data validation counts are correct\n",
    "assert(len(set(corpus)) - len(unique_docs_in_qrels) == docs_minus_qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New corpus size: 40724\n",
      "New query count: 400\n"
     ]
    }
   ],
   "source": [
    "# remove documents without query relations - they will be of no use for evaluation\n",
    "corpus = {k: v for k, v in corpus.items() if k in unique_docs_in_qrels}\n",
    "print(f\"New corpus size: {len(corpus)}\")\n",
    "\n",
    "# remove queries without query relations - they will be of no use for evaluation\n",
    "queries = {k: v for k, v in queries.items() if k in set(qrels)}\n",
    "print(f\"New query count: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to be used for evaluation\n",
    "upload_to_cloud_storage(data=queries, bucket_name = GCS_BUCKET, folder_path=GCS_FOLDER_PATH, fname=\"queries.json\")\n",
    "upload_to_cloud_storage(data=qrels, bucket_name = GCS_BUCKET, folder_path=GCS_FOLDER_PATH, fname=\"qrels.json\")\n",
    "upload_to_cloud_storage(data=corpus, bucket_name = GCS_BUCKET, folder_path=GCS_FOLDER_PATH, fname=\"corpus.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
